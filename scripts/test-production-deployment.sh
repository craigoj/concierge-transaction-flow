#!/bin/bash

# Production Deployment Testing Script
# This script validates the production deployment pipeline without actually deploying

set -e

echo "üß™ Testing Production Deployment Pipeline..."

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Test results tracking
TESTS_PASSED=0
TESTS_FAILED=0

# Test tracking functions
test_start() {
  echo -e "${BLUE}üîç Testing: $1${NC}"
}

test_pass() {
  echo -e "${GREEN}‚úÖ $1${NC}"
  ((TESTS_PASSED++))
}

test_fail() {
  echo -e "${RED}‚ùå $1${NC}"
  ((TESTS_FAILED++))
}

test_warning() {
  echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"
}

# 1. Environment Configuration Tests
test_environment_config() {
  test_start "Environment Configuration"
  
  # Check if production env files exist
  if [[ -f ".env.production" ]]; then
    test_pass "Production environment file exists"
  else
    test_fail "Production environment file missing"
  fi
  
  # Check if staging env files exist
  if [[ -f ".env.staging" ]]; then
    test_pass "Staging environment file exists"
  else
    test_fail "Staging environment file missing"
  fi
  
  # Validate environment schemas
  if npm run env:validate 2>/dev/null; then
    test_pass "Environment validation schema works"
  else
    test_warning "Environment validation may have issues (non-critical)"
  fi
}

# 2. Dependency and Security Tests
test_dependencies() {
  test_start "Dependencies and Security"
  
  # Check for vulnerabilities
  if npm audit --audit-level=high 2>/dev/null; then
    test_pass "No high-severity vulnerabilities found"
  else
    test_fail "High-severity vulnerabilities detected"
  fi
  
  # Check for outdated packages
  npm outdated > /tmp/outdated.log 2>&1 || true
  if [[ -s /tmp/outdated.log ]]; then
    test_warning "Some packages are outdated (see: npm outdated)"
  else
    test_pass "All packages are up to date"
  fi
  
  # Test dependency installation
  if npm ci --silent; then
    test_pass "Dependencies install successfully"
  else
    test_fail "Dependency installation failed"
  fi
}

# 3. Build Tests
test_build() {
  test_start "Build Process"
  
  # Set production-like environment variables for testing
  export NODE_ENV=production
  export VITE_SUPABASE_URL="https://test.supabase.co"
  export VITE_SUPABASE_ANON_KEY="test_key_minimum_100_characters_long_for_validation_to_pass_this_should_be_long_enough"
  export VITE_APP_NAME="Concierge Transaction Flow"
  export VITE_APP_VERSION="test-1.0.0"
  export VITE_APP_URL="https://test.com"
  export VITE_ENABLE_ANALYTICS=true
  export VITE_ENABLE_DEBUG=false
  export VITE_ENABLE_SENTRY=true
  export VITE_SENTRY_DSN="https://test@sentry.io/123456"
  export VITE_ENCRYPTION_KEY="test_encryption_key_32_chars_long"
  
  # Test build process
  if npm run build; then
    test_pass "Production build succeeds"
  else
    test_fail "Production build failed"
  fi
  
  # Check build output
  if [[ -d "dist" ]] && [[ -f "dist/index.html" ]]; then
    test_pass "Build output directory and files exist"
  else
    test_fail "Build output missing or incomplete"
  fi
  
  # Check build size
  BUILD_SIZE=$(du -sh dist 2>/dev/null | cut -f1 || echo "unknown")
  echo -e "${BLUE}üì¶ Build size: $BUILD_SIZE${NC}"
  
  # Clean up test build
  rm -rf dist
}

# 4. Test Suite Execution
test_test_suite() {
  test_start "Test Suite"
  
  # Run unit tests
  if npm run test:run; then
    test_pass "Unit tests pass"
  else
    test_fail "Unit tests failed"
  fi
  
  # Run integration tests (if available)
  if npm run test:integration:run 2>/dev/null; then
    test_pass "Integration tests pass"
  else
    test_warning "Integration tests not available or failed"
  fi
}

# 5. Linting and Type Checking
test_code_quality() {
  test_start "Code Quality"
  
  # Run ESLint
  if npm run lint; then
    test_pass "ESLint checks pass"
  else
    test_fail "ESLint checks failed"
  fi
  
  # Run TypeScript type checking
  if npx tsc --noEmit; then
    test_pass "TypeScript type checking passes"
  else
    test_fail "TypeScript type checking failed"
  fi
  
  # Check for TODO/FIXME comments in production code
  TODO_COUNT=$(grep -r "TODO\|FIXME" src/ --exclude-dir=node_modules 2>/dev/null | wc -l || echo "0")
  if [[ $TODO_COUNT -gt 0 ]]; then
    test_warning "$TODO_COUNT TODO/FIXME comments found in source code"
  else
    test_pass "No TODO/FIXME comments in source code"
  fi
}

# 6. Docker Configuration Tests (if applicable)
test_docker() {
  test_start "Docker Configuration"
  
  if [[ -f "Dockerfile" ]]; then
    # Validate Dockerfile syntax
    if docker build -t test-image --dry-run . 2>/dev/null; then
      test_pass "Dockerfile syntax is valid"
    else
      test_fail "Dockerfile has syntax errors"
    fi
  else
    test_warning "No Dockerfile found (skipping Docker tests)"
  fi
}

# 7. GitHub Actions Workflow Validation
test_github_actions() {
  test_start "GitHub Actions Workflows"
  
  if [[ -f ".github/workflows/ci-cd.yml" ]]; then
    # Basic YAML validation
    if yaml-lint .github/workflows/ci-cd.yml 2>/dev/null || python3 -c "import yaml; yaml.safe_load(open('.github/workflows/ci-cd.yml'))" 2>/dev/null; then
      test_pass "CI/CD workflow YAML is valid"
    else
      test_warning "CI/CD workflow YAML validation inconclusive"
    fi
  else
    test_fail "CI/CD workflow file missing"
  fi
  
  # Check for required secrets documentation
  if [[ -f "docs/github-secrets-setup.md" ]]; then
    test_pass "GitHub Secrets documentation exists"
  else
    test_warning "GitHub Secrets documentation missing"
  fi
}

# 8. Security Configuration Tests
test_security() {
  test_start "Security Configuration"
  
  # Check if .env files are in .gitignore
  if grep -q "\.env" .gitignore 2>/dev/null; then
    test_pass "Environment files are properly ignored in git"
  else
    test_fail "Environment files may not be properly ignored in git"
  fi
  
  # Check for hardcoded secrets (basic check)
  if grep -r "password\|secret\|key" src/ --include="*.ts" --include="*.tsx" | grep -v "import\|interface\|type\|comment" | grep -q "=.*['\"].*['\"]"; then
    test_warning "Potential hardcoded secrets found (manual review needed)"
  else
    test_pass "No obvious hardcoded secrets found"
  fi
  
  # Check security headers configuration
  if [[ -f "vercel.json" ]] && grep -q "X-Content-Type-Options" vercel.json; then
    test_pass "Security headers configured"
  else
    test_warning "Security headers configuration not found"
  fi
}

# 9. Production Readiness Checklist
test_production_readiness() {
  test_start "Production Readiness Checklist"
  
  local readiness_items=(
    "Environment configuration:completed"
    "Monitoring setup:completed"
    "Error tracking:completed"
    "Email service:completed"
    "Security configuration:completed"
    "Backup procedures:pending"
    "Load testing:pending"
    "Performance optimization:pending"
  )
  
  for item in "${readiness_items[@]}"; do
    IFS=':' read -r name status <<< "$item"
    case $status in
      "completed")
        test_pass "$name"
        ;;
      "pending")
        test_warning "$name (pending)"
        ;;
      *)
        test_fail "$name (unknown status)"
        ;;
    esac
  done
}

# 10. Performance Testing
test_performance() {
  test_start "Performance Validation"
  
  # Build for performance testing
  if npm run build > /dev/null 2>&1; then
    # Check bundle size
    if [[ -f "dist/assets/index.*.js" ]]; then
      BUNDLE_SIZE=$(ls -la dist/assets/index.*.js | awk '{print $5}')
      BUNDLE_SIZE_MB=$((BUNDLE_SIZE / 1024 / 1024))
      
      if [[ $BUNDLE_SIZE_MB -lt 1 ]]; then
        test_pass "Bundle size is reasonable (${BUNDLE_SIZE_MB}MB)"
      else
        test_warning "Bundle size is large (${BUNDLE_SIZE_MB}MB) - consider optimization"
      fi
    fi
    
    # Clean up
    rm -rf dist
  else
    test_warning "Cannot test performance - build failed"
  fi
}

# Generate deployment summary
generate_summary() {
  echo ""
  echo "=============================================="
  echo -e "${BLUE}üìã PRODUCTION DEPLOYMENT TEST SUMMARY${NC}"
  echo "=============================================="
  echo -e "${GREEN}‚úÖ Tests Passed: $TESTS_PASSED${NC}"
  echo -e "${RED}‚ùå Tests Failed: $TESTS_FAILED${NC}"
  echo ""
  
  if [[ $TESTS_FAILED -eq 0 ]]; then
    echo -e "${GREEN}üéâ Production deployment pipeline is ready!${NC}"
    echo -e "${GREEN}‚úÖ All critical tests passed${NC}"
    echo ""
    echo "Next steps:"
    echo "1. Set up production environment variables in GitHub Secrets"
    echo "2. Configure production Supabase project"
    echo "3. Set up production monitoring and alerts"
    echo "4. Perform final manual testing"
    echo "5. Deploy to production!"
  elif [[ $TESTS_FAILED -le 2 ]]; then
    echo -e "${YELLOW}‚ö†Ô∏è  Production deployment has minor issues${NC}"
    echo -e "${YELLOW}Please review failed tests before deploying${NC}"
  else
    echo -e "${RED}üö´ Production deployment is NOT ready${NC}"
    echo -e "${RED}Please fix failed tests before proceeding${NC}"
  fi
  
  echo ""
  echo "For more information, see:"
  echo "- docs/github-secrets-setup.md"
  echo "- docs/staging-environment.md"
  echo "- .github/workflows/ci-cd.yml"
}

# Main execution
main() {
  echo -e "${BLUE}üöÄ Production Deployment Pipeline Testing${NC}"
  echo "============================================="
  echo ""
  
  test_environment_config
  echo ""
  test_dependencies
  echo ""
  test_build
  echo ""
  test_test_suite
  echo ""
  test_code_quality
  echo ""
  test_docker
  echo ""
  test_github_actions
  echo ""
  test_security
  echo ""
  test_production_readiness
  echo ""
  test_performance
  
  generate_summary
}

# Run main function
main "$@"